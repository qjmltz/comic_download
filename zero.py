import requests
from bs4 import BeautifulSoup
import os
from download import download_images
# from playwright.sync_api import sync_playwright
# import time
headers = {
    "User-Agent": "Mozilla/5.0"
}
cookies = {}


def set_cookie(cookie_input):
    global cookies
    if isinstance(cookie_input, str):
        # è§£æç±»ä¼¼ "key1=value1; key2=value2" çš„å­—ç¬¦ä¸²
        cookie_dict = {}
        for part in cookie_input.split(";"):
            if "=" in part:
                k, v = part.strip().split("=", 1)
                cookie_dict[k] = v
        cookies = cookie_dict
    elif isinstance(cookie_input, dict):
        cookies = cookie_input


def get_comic_name(comic_id):
    url = f"https://www.zerobywai.com/pc/manga_pc.php?kuid={comic_id}"
    resp = requests.get(url, headers=headers, cookies=cookies)
    if resp.status_code != 200:
        raise Exception("è·å–é¡µé¢å¤±è´¥")

    soup = BeautifulSoup(resp.text, "html.parser")

    # æ–°æ ‡é¢˜ä½ç½®
    name_tag = soup.select_one("h1.text-2xl")
    if not name_tag:
        raise Exception("æœªæ‰¾åˆ°æ¼«ç”»å")

    # å¤„ç† <br>ï¼Œå¹¶å»æ‰å¤šä½™è¯´æ˜
    raw_title = name_tag.get_text(separator="\n").strip()
    title_lines = [line.strip() for line in raw_title.split("\n") if line.strip()]

    # ä¸€èˆ¬ç¬¬ä¸€è¡Œå°±æ˜¯ä¸»æ ‡é¢˜
    comic_name = title_lines[0]

    return comic_name


def get_chapter_links(comic_id):
    url = f"https://www.zerobywai.com/pc/manga_pc.php?kuid={comic_id}"
    resp = requests.get(url, headers=headers, cookies=cookies)
    if resp.status_code != 200:
        raise Exception(f"ç« èŠ‚é¡µé¢åŠ è½½å¤±è´¥ï¼š{resp.status_code}")

    soup = BeautifulSoup(resp.text, "html.parser")
    chapter_links = []

    for a in soup.select('a[href*="manga_read_pc.php?zjid="]'):
        href = a.get("href")
        title = a.text.strip()

        if not href or not title:
            continue

        # è¡¥å…¨ä¸ºç»å¯¹åœ°å€
        full_url = "https://www.zerobywai.com/pc/" + href.lstrip("/")

        chapter_links.append((full_url, f"ç¬¬{title}è¯"))

    if not chapter_links:
        raise Exception("æœªæ‰¾åˆ°ä»»ä½•ç« èŠ‚é“¾æ¥ï¼Œé¡µé¢ç»“æ„å¯èƒ½å†æ¬¡å˜åŒ–")

    return chapter_links


def download_chapter(url, title, save_root):
    folder = os.path.join(save_root, title)
    print(f"\nğŸ“– ä¸‹è½½ç« èŠ‚ã€Š{title}ã€‹")

    resp = requests.get(url, headers=headers, cookies=cookies)
    if resp.status_code != 200:
        print("âŒ ç« èŠ‚é¡µé¢åŠ è½½å¤±è´¥")
        return

    soup = BeautifulSoup(resp.text, "html.parser")

    img_urls = []
    for img in soup.select("img.manga-image"):
        src = img.get("src")
        if not src:
            continue

        # å¤„ç† // å¼€å¤´çš„åè®®ç›¸å¯¹è·¯å¾„
        if src.startswith("//"):
            src = "https:" + src

        img_urls.append(src)

    if not img_urls:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå›¾ç‰‡")
        return

    download_images(img_urls, folder, headers)