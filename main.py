import argparse
import importlib
import os
import json

def parse_chapter_range(chap_input, total_count):
    if chap_input == "all":
        return list(range(total_count))
    elif "-" in chap_input:
        start, end = map(int, chap_input.split("-"))
        return list(range(start - 1, min(end, total_count)))
    else:
        idx = int(chap_input) - 1
        return [idx] if 0 <= idx < total_count else []

def crawl(comic_id, site_module, chapter_range="all", save_dir="./downloads", cookies=None):
    # å¦‚æœç½‘ç«™æ¨¡å—æ”¯æŒè®¾ç½® cookieï¼Œåˆ™è°ƒç”¨
    if cookies and hasattr(site_module, "set_cookie"):
        site_module.set_cookie(cookies)

    # å°è¯•è·å–æ¼«ç”»åç§°ï¼Œå¤±è´¥å°±ç”¨comic_id
    try:
        comic_name = site_module.get_comic_name(comic_id)
        print(f"ğŸ‰ è·å–æ¼«ç”»åç§°æˆåŠŸï¼š{comic_name}")
    except Exception as e:
        print(f"âš ï¸ è·å–æ¼«ç”»åç§°å¤±è´¥ï¼Œä½¿ç”¨æ¼«ç”»IDä»£æ›¿: {comic_id}\né”™è¯¯: {e}")
        comic_name = comic_id

    chapters = site_module.get_chapter_links(comic_id)
    total = len(chapters)
    indexes = parse_chapter_range(chapter_range, total)

    print(f"ğŸ“š å…± {total} ç« ï¼Œå‡†å¤‡ä¸‹è½½ {len(indexes)} ç« ï¼š{[i+1 for i in indexes]}")

    for i in indexes:
        url, title = chapters[i]
        print(f"ğŸ“¥ ä¸‹è½½ç¬¬ {i+1} ç«  - {title}")
        # ç”¨æ¼«ç”»åç§°ä½œä¸ºæ–‡ä»¶å¤¹å
        site_module.download_chapter(url, title, os.path.join(save_dir, comic_name))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ğŸ§² æ¼«ç”»ä¸‹è½½å™¨")
    parser.add_argument("site", help="ç½‘ç«™ä»£å·ï¼Œä¾‹å¦‚ manhuazhan")
    parser.add_argument("comic_id", help="æ¼«ç”» IDï¼Œä¾‹å¦‚ 235514")
    parser.add_argument("--chapter", default="all", help="ä¸‹è½½ç« èŠ‚ç¼–å·ï¼ˆä¾‹å¦‚ all æˆ– 8 æˆ– 8-60ï¼‰")
    parser.add_argument("--cookie", default=None, help="Cookie å­—ç¬¦ä¸²æˆ– JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--save_dir", default="./downloads", help="ä¿å­˜ç›®å½•")

    args = parser.parse_args()

    # åŠ¨æ€å¯¼å…¥æ¨¡å—
    try:
        site_module = importlib.import_module(f"sites.{args.site}")
    except ModuleNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°ç½‘ç«™æ¨¡å—ï¼šsites.{args.site}")
        exit(1)

    # è§£æ Cookie
    cookies = None
    if args.cookie:
        if os.path.exists(args.cookie):
            try:
                with open(args.cookie, "r", encoding="utf-8") as f:
                    parsed = json.load(f)
                    # æ”¯æŒæµè§ˆå™¨å¯¼å‡ºçš„ list[dict]
                    if isinstance(parsed, list):
                        cookies = {item["name"]: item["value"] for item in parsed if "name" in item and "value" in item}
                        print(f"âœ… æˆåŠŸè¯»å– cookie æ–‡ä»¶ï¼ˆæµè§ˆå™¨å¯¼å‡ºæ ¼å¼ï¼‰ï¼Œå…± {len(cookies)} æ¡")
                    elif isinstance(parsed, dict):
                        cookies = parsed
                        print(f"âœ… æˆåŠŸè¯»å– cookie æ–‡ä»¶ï¼ˆå­—å…¸æ ¼å¼ï¼‰ï¼Œå…± {len(cookies)} æ¡")
                    else:
                        print(f"âš ï¸ æ— æ³•è¯†åˆ«çš„ cookie æ–‡ä»¶æ ¼å¼ï¼š{args.cookie}")
            except Exception as e:
                print(f"âŒ è¯»å– cookie æ–‡ä»¶å¤±è´¥ï¼š{e}")
        else:
            # ç›´æ¥ä¼ å…¥çš„æ˜¯ cookie å­—ç¬¦ä¸²
            cookies = args.cookie
            print(f"âœ… ä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥çš„ cookie å­—ç¬¦ä¸²")

    else:
        print("â„¹ï¸ æœªæä¾› cookieï¼Œå°†ä»¥æ¸¸å®¢èº«ä»½è®¿é—®")

    crawl(args.comic_id, site_module, args.chapter, args.save_dir, cookies)